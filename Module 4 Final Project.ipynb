{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Final Project\n",
    "\n",
    "## Time Series Analysis of Real Estate  - Comparing New York City to San Francisco\n",
    "\n",
    "The goal of this project is to provide insights and recommendations to our stakeholders, who are interested in learning what is the best return in real estate investment for a span of 2 and 5 years when deciding whether to buy on the East Coast in New York City, NY or in the West Coast in San Francisco, CA.\n",
    "\n",
    "We will recommend, based on our analysis and modeling of historical data on real estate prices, obtained from the [Zillow Research Page](https://www.zillow.com/research/data/), what is the most sound financial decision, and we will also provide the 5 best zipcodes for buyers today in both cities for each investment time lag.\n",
    "\n",
    "\n",
    "## Part I - Comparing San Francisco and NYC\n",
    "\n",
    "\n",
    "### 1. Load libraries and data\n",
    "\n",
    "We will start by loading all the necessary libraries, as well as our data from the file that has already been saved to this repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly as py\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pyramid.arima import auto_arima\n",
    "from pyramid.arima.utils import ndiffs\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "df = pd.read_csv('zillow_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking how many unique values we have on each column, besides the time data\n",
    "df.iloc[:,0:7].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our dataframe we can see that the time data is formatted as columns. We have both RegionID and RegionName, and a quick research online shows that the column RegionName is the one that refers to zipcodes. Let's rename it accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming column that refers to zipcodes\n",
    "df.rename({'RegionName': 'Zipcode'}, axis='columns', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have data for the entire country. Let's filter out only what we will need - New York City and San Francisco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering only data for New York City and San Francisco\n",
    "san_francisco = df.loc[df['City'] == 'San Francisco']\n",
    "nyc = df.loc[df['City'] == 'New York']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "san_francisco.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to check how many unique zipcodes we have in each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking total number of zip codes for SF\n",
    "print(f'We have {san_francisco.Zipcode.nunique()} different zip codes in our dataset for San Francisco.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking total number of zip codes for NY\n",
    "print(f'We have {nyc.Zipcode.nunique()} different zip codes in our dataset for NYC.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have a total of 133 unique zipcodes for our time series analysis. \n",
    "\n",
    "Our next step will be to use a function to transform our data from a wide format as it is now to a long format, and then make it into a pandas series of datetime object. We were provided with this helper function from our curriculum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create function to melt data and make it into time series\n",
    "\n",
    "def melt_data(df):\n",
    "    ''' \n",
    "    Takes a dataframe with datetime data that is in wide format and melts it into long format; \n",
    "    Tranforms data into datetime object with time as index.\n",
    "    User will need to change columns names on first line of code according to their own dataframe.\n",
    "    '''\n",
    "    \n",
    "    melted = pd.melt(df, id_vars=['RegionID', 'Zipcode', 'City', 'State', 'Metro', 'CountyName', 'SizeRank'], var_name='time')\n",
    "    melted['time'] = pd.to_datetime(melted['time'], infer_datetime_format=True)\n",
    "    melted = melted.dropna(subset=['value'])\n",
    "    return melted.groupby('time').aggregate({'value':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply function to our data:\n",
    "sf_ts = melt_data(san_francisco)\n",
    "nyc_ts = melt_data(nyc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now our data is in a time series format with a DatetimeIndex and a column with our target values with a total of 252 datapoints, which correspond to 21 years. We can now move on to some visualizations of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualizations\n",
    "\n",
    "Now that we have our data in the right format for our two cities, let's start by looking at some visualizations to help us deepen our understanding and direct our analysis and modeling strategy.\n",
    "\n",
    "Let's first of all do a quick visualization of this data, comparing both cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=sf_ts.index, y=sf_ts.value, name='San Francisco',\n",
    "                         line_color='magenta'))\n",
    "fig.add_trace(go.Scatter(x=nyc_ts.index, y=nyc_ts.value, name='New York City',\n",
    "                         line_color='deepskyblue'))\n",
    "fig.update_layout(title_text='San Francisco & NYC Zillow Home Value Index',\n",
    "                  xaxis_rangeslider_visible=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is very interesting. We can see from our graph that the mean Zillow Home Value Index (ZHVI) for the city of San Francisco, CA is higher than that for New York City, NY. We can also note the trends in the data for both cities: overall continuously rising trend, with some drop following the 2007-2008 real estate crisis - less so for NYC than for San Francisco. At the same time, we note that the upward trend is steeper for San Francisco during the last years - which makes sense when one thinks of the impact that the technology companies have been causing in that area.\n",
    "\n",
    "We also note a big straight jump in the data for NYC in 2004, and upon investigating we see that this is because several zipcodes were created in that year. This inconsistency could make our modeling and predictions less robust but at the same time we could lose important information by eliminating zipcodes that could be rentable today, so we will choose to keep it that way for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From simply inspecting our data we can tell it is not stationary (positive trend), but we can go further into evaluating our data for trends and seasonality. We will create a function to calculate and plot rolling statistics for mean and standard deviation, as well as perform a Dickey-Fuller test for stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to plot time series with rolling mean and rolling variance, plus results from Dickey-Fuller test\n",
    "\n",
    "def plot_roll(ts, name=''):\n",
    "    \n",
    "    '''Takes a time series and plots it along with its rolling mean and its rolling standard deviation.\n",
    "    Calculates and prints results from Dickey-Fuller test.'''\n",
    "    \n",
    "    # Calculate rolling mean and rolling standard deviation:\n",
    "    rolmean = ts.rolling(window = 6, center = False).mean()\n",
    "    rolstd = ts.rolling(window = 6, center = False).std()\n",
    "    \n",
    "    # Plot original time series and its rolling mean/standard deviation\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=ts.index, y=ts.value, name='Original',\n",
    "                             line_color='blue'))\n",
    "    fig.add_trace(go.Scatter(x=rolmean.index, y=rolmean.value, name='Rolling Mean',\n",
    "                             line_color='red'))\n",
    "    fig.add_trace(go.Scatter(x=rolstd.index, y=rolstd.value, name='Rolling Std',\n",
    "                             line_color='green'))\n",
    "    fig.update_layout(title_text=f'{name} Rolling Mean & Standard Deviation',\n",
    "                      xaxis_rangeslider_visible=True)\n",
    "    fig.show();\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print (f'{name} Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(ts['value'])\n",
    "\n",
    "    # Extract and display test results in a user friendly manner\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    return print(dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checking stationarity for San Francisco\n",
    "sf_dfoutput = plot_roll(sf_ts, 'San Francisco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking stationarity for NYC\n",
    "plot_roll(nyc_ts, 'New York')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our rolling statistics plots we can note that even though we can't easily observe a trend or seasonality in our standard deviation, the mean is notably increasing so our data is clearly not stationary. Of course the results from the Dickey-Fuller tests also confirm this, for both cities. We will, further ahead, need to make it stationary in order to model it correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a yearly dataframe so that we check for any seasonality throughout the years. For this we'll create a function that takes a time series and returns a plot with the yearly grouped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new DataFrame with yearly values stored in columns \n",
    "\n",
    "def plot_yearly_ts(ts, name='', boxplot=False):\n",
    "    \n",
    "    '''Function takes a time series and groups it into yearly intervals using Grouper;\n",
    "    Creates a new dataframe where yearly values are into columns to faciliate plotting.\n",
    "    Gives the option to create a boxplot instead of line plot, if parameter is passed as True.'''\n",
    "    \n",
    "    # group data by year and made into dataframe\n",
    "    year_group = ts.groupby(pd.Grouper(freq ='A'))\n",
    "    yearly = pd.DataFrame()\n",
    "    for year, group in year_group:\n",
    "        yearly[year.year] = group.values.ravel()\n",
    "        \n",
    "    # plot boxplot if option is True\n",
    "    if boxplot:\n",
    "        ax = yearly.boxplot(figsize = (18,10))\n",
    "        ax.set_ylabel('Value($)')\n",
    "        ax.set_xlabel('Year')\n",
    "        plt.title(f'{name} Mean Zillow Home Value Index (ZHVI) - Yearly', fontsize=18);\n",
    "        \n",
    "   # otherwise plot line plot\n",
    "    else:\n",
    "        plt.figure(figsize = (14,10))\n",
    "        ax = plt.subplot(111)\n",
    "        # speficy colors for plots\n",
    "        tableau20 = [(0.12156862745098039, 0.4666666666666667, 0.7058823529411765), (0.6823529411764706, 0.7803921568627451, 0.9098039215686274), (1.0, 0.4980392156862745, 0.054901960784313725), (1.0, 0.7333333333333333, 0.47058823529411764), (0.17254901960784313, 0.6274509803921569, 0.17254901960784313), (0.596078431372549, 0.8745098039215686, 0.5411764705882353), (0.8392156862745098, 0.15294117647058825, 0.1568627450980392), (1.0, 0.596078431372549, 0.5882352941176471), (0.5803921568627451, 0.403921568627451, 0.7411764705882353), (0.7725490196078432, 0.6901960784313725, 0.8352941176470589), (0.5490196078431373, 0.33725490196078434, 0.29411764705882354), (0.7686274509803922, 0.611764705882353, 0.5803921568627451), (0.8901960784313725, 0.4666666666666667, 0.7607843137254902), (0.9686274509803922, 0.7137254901960784, 0.8235294117647058), (0.4980392156862745, 0.4980392156862745, 0.4980392156862745), (0.7803921568627451, 0.7803921568627451, 0.7803921568627451), (0.7372549019607844, 0.7411764705882353, 0.13333333333333333), (0.8588235294117647, 0.8588235294117647, 0.5529411764705883), (0.09019607843137255, 0.7450980392156863, 0.8117647058823529), (0.6196078431372549, 0.8549019607843137, 0.8980392156862745), (0.6823529411764706, 0.7803921568627451, 0.9098039215686274), (1.0, 0.4980392156862745, 0.054901960784313725), (1.0, 0.7333333333333333, 0.47058823529411764)]\n",
    "        # remove borders from plot\n",
    "        ax.spines[\"top\"].set_visible(False)  \n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        n_points = yearly.shape[0]\n",
    "        for rank, year in enumerate(yearly):\n",
    "            ax.plot(np.arange(n_points-(n_points-1), n_points+1), yearly[year].values, color=tableau20[rank], lw=2)\n",
    "            plt.xlim(1, 12)\n",
    "            y_pos = yearly[year].values[-1] \n",
    "            ax.text(n_points, y_pos, year, color=tableau20[rank], fontsize=12)\n",
    "            ax.grid(which='major', axis='y', linestyle= \"--\", lw=0.5, color=\"black\", alpha=0.3)\n",
    "            ax.set_ylabel('Value($)')\n",
    "            ax.set_xlabel('Month')\n",
    "            plt.title(f'{name} Mean Zillow Home Value Index (ZHVI) - Yearly', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data\n",
    "sf_ts.head(), sf_ts.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While trying to run our function we were getting errors and upon investigating our data further, we noticed that both years 1996 and 2018 are not complete years, so we will filter out these years and keep only the years that have full monthly data for the yearly plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out years that are not complete\n",
    "sf_ts_filter = sf_ts['1997':'2017']\n",
    "nyc_ts_filter = nyc_ts['1997':'2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot yearly data for San Francisco\n",
    "plot_yearly_ts(sf_ts_filter, 'San Francisco')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a lot of data in one plot, but it is useful to note a few things. We can't observe any clear seasonality in the yearly data, and we can see that the mean steadily moves up as the years increase, as we were able to observe in the previous plot, except for the years between 2009 and 2011 which reflect the impact of the financial crisis. Let's do a second visualization of only those years during the housing bubble crisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot other years for San Francisco for the time of the financial crisis\n",
    "plot_yearly_ts(sf_ts_filter['2007':'2012'], 'San Francisco')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in this plot we can see that, starting at the end of 2007 home values started to fall and continued to do so  for the next 4 years, until about the end of 2011 when the trend starts to move up again. It was a slow fall and a quick recovery - values were at the lowest around August 2011 and by the end of 2012 house values are almost at the same level they were at the start of the crisis.\n",
    "\n",
    "Let's also look at the boxplots for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boxplots for San Francisco\n",
    "plot_yearly_ts(sf_ts_filter, 'San Francisco', boxplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot further confirms that there is a positive trend in the data. It also shows that the data variance is not changing by much year by year, and there is less variance coinciding with the years with more of a flat trend. We can again see that the mean is rising, except for the years between 2008 and 2011. There are also not many outliers in our data.  \n",
    "\n",
    "Let's also have a look at New York City."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot yearly data for NYC \n",
    "plot_yearly_ts(nyc_ts_filter, 'New York City')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we see that there doesn't seem to be any clear seasonality in our data. As with San Francisco, the values steadily increase except for the period between 2006 and 2012 when values are much closer together. Let's again have a zoomed up look at these years only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot other years data for NYC\n",
    "plot_yearly_ts(nyc_ts_filter['2006':'2012'], 'New York City')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can obserse is that for NYC the effects of the housing crisis were less steady. We see how by middle of 2006 prices were rising up (as it was with the bubble for most locations) and then start to fall quickly, but by end of 2007 values were rising again, only to fall in 2008 - but never as low as they were in the end of 2006. By 2013 the recovery is clear and seems exponential. Overall this tells us that house values in NYC fared better during the crisis overall than that of San Francisco.\n",
    "\n",
    "Let's also look at the boxplot for the New York City data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boxplots for NYC\n",
    "plot_yearly_ts(nyc_ts_filter, 'New York City', boxplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The New York City data looks similar, and we can note that the financial crisis had less impact for NYC. The values steadily rise from year to year with some stagnation in the wake of the house bubble crisis, from years 2009 to 2012. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate Returns Statistics\n",
    "\n",
    "We want to create some statistics to help us better evaluate returns and eventual losses in times of crises for both cities, which is what our stakeholders are interested in. \n",
    "\n",
    "We'll create a function in order to do that. Our function will calculate what was the percentage gain over amount invested in a lag of 2, 5 and 10 years previous, throughout the time, plot and return the information as dataframes for further investigation and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate gain percentage (ROI) on previous 2, 5 and 10 years\n",
    "def calculate_gain(ts, plot=True):\n",
    "    \n",
    "    '''Takes a real estate time series and performs calculations on returns over investments\n",
    "    for periods of 2, 5 and 10 years.'''\n",
    "    \n",
    "    # calculates ROI by taking current value, decreasing investment(value at x steps in past) and \n",
    "    # dividing by investment. Multiplies by 100 to get percentage number\n",
    "    roi_2 = (ts - ts.shift(periods=24))/ts.shift(periods=24)*100\n",
    "    roi_5 = (ts - ts.shift(periods=60))/ts.shift(periods=60)*100\n",
    "    roi_10 = (ts - ts.shift(periods=120))/ts.shift(periods=120)*100\n",
    "    roi_2.dropna(inplace=True)\n",
    "    roi_5.dropna(inplace=True)\n",
    "    roi_10.dropna(inplace=True)\n",
    "    \n",
    "    # plot results\n",
    "    if plot:\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=roi_2.index, y=roi_2.value, name='2-Year Investment',\n",
    "                                 line_color='deepskyblue'))\n",
    "        fig.add_trace(go.Scatter(x=roi_5.index, y=roi_5.value, name='5-Year Investment',\n",
    "                                 line_color='red'))\n",
    "        fig.add_trace(go.Scatter(x=roi_10.index, y=roi_10.value, name='10-Year Investment',\n",
    "                                 line_color='gray'))\n",
    "        fig.update_layout(title_text='Mean Zillow Home Value Index (ZHVI) - ROI Over Time Invested',\n",
    "                          xaxis_rangeslider_visible=True)\n",
    "        fig.show()\n",
    "    \n",
    "    # saves new ts with returns for 2, 5 and 10 periods\n",
    "    return roi_2, roi_5, roi_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's call the function to examine San Francisco's data first\n",
    "sf_roi2, sf_roi5, sf_roi10 = calculate_gain(sf_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the returns for investment with a time lag of 10 years are potentially the highest - they peaked at just over 200% at around 2007 in San Francisco for the people who had bought 10 years before. It is also at the highest fluctuation, as it comes down quickly form over 200% to around 20% in a span of approximately 5 years. It is however always a positive return, never under 20% even at the height of financial crisis.\n",
    "\n",
    "On the other hand, on a 2 and 5 lag investment, there is loss during the financial crisis, from around 2008 to 2012-2013, that reaches a little over 12% of loss. Once over the crisis the gains rise again, but are now in what looks a downward trend. \n",
    "\n",
    "Let's see how NYC fares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nyc_roi2, nyc_roi5, nyc_roi10 = calculate_gain(nyc_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NYC one thing we notice is the effect of that quick drop that shows the period in time (around 2004) with the creation of new zipcodes. Regardless of this, what we can see is that investing in NYC has rarely represented a loss, even at the peak of the housing crisis as we have only a short period of loss around 2010 and 2013. The returns on 10 year-investments also seem highest for NYC, reaching over 260% at the initial point, and even at the lowest return in 10-years it is around 40% and thus higher when compared to the lowest 10-year return for San Francisco (of around 20%).\n",
    "\n",
    "New York also seems to be starting to have a downward trend in terms of return of investment, although it doesn't seem as pronounced as for San Francisco.\n",
    "\n",
    "We want to further compare the returns for both cities, side-by-side. We'll create a quick function to return the max and min return for each city for each investment lag.\n",
    "\n",
    "In addition we want to plot the cities side-by-side so that we can better visualize and compare their performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create visualization of both time series:\n",
    "plt.figure(figsize=(18,6))\n",
    "ax = plt.subplot(111)  \n",
    "ax.spines[\"top\"].set_visible(False)  \n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.plot(sf_roi10, label='San Francisco', color='magenta')\n",
    "ax.plot(nyc_roi10, label='NYC', color='blue')\n",
    "ax.fill_between(sf_roi10.index, sf_roi10.value<0, -20, color='red', alpha=.2)\n",
    "plt.title('San Francisco vs. NYC - Return on 10 year Investment', fontsize=16)\n",
    "ax.set_ylabel('Gain Percentage')\n",
    "ax.set_xlabel('Year')\n",
    "plt.yticks([-10,0,10,25,50,75,100,125,150,175,200, 250], [str(x) + \"%\" for x in [-10,0,10,25,50,75,100,125,150,175,200, 250]], fontsize=10)\n",
    "ax.grid(which='major', axis='y', linestyle= \"--\", lw=0.5, color=\"black\", alpha=0.3)\n",
    "ax.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like over time investing in NYC has brought higher returns, and this only reverses more recently, from around 2015, which is a clear reflection of the current real estate situation in San Francisco, where there is a big housing shortage following the tech and Internet job boom in that area. From [Wikipedia](https://en.wikipedia.org/wiki/San_Francisco_housing_shortage): \"from 2012 to 2016, the San Francisco metropolitan area added 373,000 new jobs, but permitted only 58,000 new housing units.\"\n",
    "\n",
    "Even so, the return in investment for both cities is very close at current times. However, we must point out that the mean house price for New York City is less than for San Francisco, meaning that less money invested is needed. Let's see how they compare for 2-year and 5-year investments over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create visualization of both time series:\n",
    "plt.figure(figsize=(18,6))\n",
    "ax = plt.subplot(111)  \n",
    "ax.spines[\"top\"].set_visible(False)  \n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.plot(sf_roi5, label='San Francisco', color='magenta')\n",
    "ax.plot(nyc_roi5, label='NYC', color='blue')\n",
    "ax.fill_between(sf_roi5.index, sf_roi5.value<0, -20, color='red', alpha=.2)\n",
    "plt.title('San Francisco vs. NYC - Return on 5 year Investment', fontsize=16)\n",
    "ax.set_ylabel('Gain Percentage')\n",
    "ax.set_xlabel('Year')\n",
    "plt.yticks([-10,0,10,25,50,75,100,125,150,175,200, 250], [str(x) + \"%\" for x in [-10,0,10,25,50,75,100,125,150,175,200, 250]], fontsize=10)\n",
    "ax.grid(which='major', axis='y', linestyle= \"--\", lw=0.5, color=\"black\", alpha=0.3)\n",
    "ax.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create visualization of both time series:\n",
    "plt.figure(figsize=(18,6))\n",
    "ax = plt.subplot(111)  \n",
    "ax.spines[\"top\"].set_visible(False)  \n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.plot(sf_roi2, label='San Francisco', color='magenta')\n",
    "ax.plot(nyc_roi2, label='NYC', color='blue')\n",
    "ax.fill_between(sf_roi2.index, sf_roi2.value<0, -20, color='red', alpha=.2)\n",
    "plt.title('San Francisco vs. NYC - Return on 2 year Investment', fontsize=16)\n",
    "ax.set_ylabel('Gain Percentage')\n",
    "ax.set_xlabel('Year')\n",
    "plt.yticks([-10,0,10,25,50,75,100,125,150,175,200, 250], [str(x) + \"%\" for x in [-10,0,10,25,50,75,100,125,150,175,200, 250]], fontsize=10)\n",
    "ax.grid(which='major', axis='y', linestyle= \"--\", lw=0.5, color=\"black\", alpha=0.3)\n",
    "ax.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see from both plots is that San Francisco started out in our time frame as a better investment, was overtaken by NYC right before the crisis, and are now more comparable, especially on a 2-year lag. \n",
    "\n",
    "Let's do a little side-by-side comparison for highest and lowest returns for each city so that we can more easily see which is the city that shows the best returns and smaller losses. I'll make another function to get this statistics comparison in an easy to read format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_gain(ts1, ts2, name='', name2='', plot=True):\n",
    "    \n",
    "    '''Takes two time series, calculates returns in 2, 5 and 10 years for each \n",
    "    and returns max and min values for each'''\n",
    "    \n",
    "    # calculates returns for first time series\n",
    "    ts1.dropna(inplace=True)\n",
    "    ts1_roi_2, ts1_roi_5, ts1_roi_10 = calculate_gain(ts1, plot=False)\n",
    "\n",
    "    \n",
    "    # calculates returns for second time series\n",
    "    ts2.dropna(inplace=True)\n",
    "    ts2_roi_2, ts2_roi_5, ts2_roi_10 = calculate_gain(ts2, plot=False)\n",
    "    \n",
    "    # organize returns values and make into dataframes\n",
    "    hist_roi = {}\n",
    "    hist_roi[name] = {}\n",
    "    hist_roi[name]['Max Return 02-Year'] = round(ts1_roi_2.values.max(),2)\n",
    "    hist_roi[name]['Min Return 02-Year'] = round(ts1_roi_2.values.min(),2)\n",
    "    hist_roi[name]['Max Return 05-Year'] = round(ts1_roi_5.values.max(),2)\n",
    "    hist_roi[name]['Min Return 05-Year'] = round(ts1_roi_5.values.min(),2)\n",
    "    hist_roi[name]['Max Return 10-Year'] = round(ts1_roi_10.values.max(),2)\n",
    "    hist_roi[name]['Min Return 10-Year'] = round(ts1_roi_10.values.min(),2)\n",
    "    \n",
    "    hist_roi[name2] = {}\n",
    "    hist_roi[name2]['Max Return 02-Year'] = round(ts2_roi_2.values.max(),2)\n",
    "    hist_roi[name2]['Min Return 02-Year'] = round(ts2_roi_2.values.min(),2)\n",
    "    hist_roi[name2]['Max Return 05-Year'] = round(ts2_roi_5.values.max(),2)\n",
    "    hist_roi[name2]['Min Return 05-Year'] = round(ts2_roi_5.values.min(),2)\n",
    "    hist_roi[name2]['Max Return 10-Year'] = round(ts2_roi_10.values.max(),2)\n",
    "    hist_roi[name2]['Min Return 10-Year'] = round(ts2_roi_10.values.min(),2)\n",
    "    \n",
    "    last_roi = {}\n",
    "    last_roi[name] = {}\n",
    "    last_roi[name]['Mean 6M Investment'] = round(ts1.iloc[-7:-1].values.mean(),0)\n",
    "    last_roi[name]['Mean 6M Return 02-Year'] = round(ts1_roi_2.iloc[-7:-1].values.mean(),2)\n",
    "    last_roi[name]['Mean 6M Return 05-Year'] = round(ts1_roi_5.iloc[-7:-1].values.mean(),2)\n",
    "    last_roi[name]['Mean 6M Return 10-Year'] = round(ts1_roi_10.iloc[-7:-1].values.mean(),2)\n",
    "    \n",
    "    last_roi[name2] = {}\n",
    "    last_roi[name2]['Mean 6M Investment'] = round(ts2.iloc[-7:-1].values.mean(),0)\n",
    "    last_roi[name2]['Mean 6M Return 02-Year'] = round(ts2_roi_2.iloc[-7:-1].values.mean(),2)\n",
    "    last_roi[name2]['Mean 6M Return 05-Year'] = round(ts2_roi_5.iloc[-7:-1].values.mean(),2)\n",
    "    last_roi[name2]['Mean 6M Return 10-Year'] = round(ts2_roi_10.iloc[-7:-1].values.mean(),2)\n",
    "           \n",
    "    hist = pd.DataFrame.from_dict(hist_roi)\n",
    "    last = pd.DataFrame.from_dict(last_roi)\n",
    "    complete = pd.concat([hist, last])\n",
    "    \n",
    "    \n",
    "    # plot results\n",
    "    if plot:\n",
    "        my_colors = [(x/10.0, x/20.0, 0.75) for x in range(1,12)]\n",
    "        ax = complete.loc[['Mean 6M Investment'], :].plot(kind='bar', colormap='Accent', figsize=(8,6))\n",
    "        ax.set_xlabel('Mean 6M Investment')\n",
    "        ax.set_xticks([])\n",
    "        plt.title('Mean Last 6 Months Investment', fontsize=16)\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        ax.spines[\"top\"].set_visible(False)  \n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        \n",
    "        gains = complete.drop(['Mean 6M Investment','Mean 6M Return 02-Year','Mean 6M Return 05-Year','Mean 6M Return 10-Year'], axis=0)\n",
    "        ax = gains.plot(kind='bar', colormap='Accent', figsize=(12,8))\n",
    "        plt.gcf().autofmt_xdate()\n",
    "        plt.title('Historical Returns', fontsize=16)\n",
    "        plt.axhline(y=0, color='red')\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        ax.spines[\"top\"].set_visible(False)  \n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        \n",
    "        ax = complete.loc[['Mean 6M Return 02-Year','Mean 6M Return 05-Year','Mean 6M Return 10-Year'], :].plot(kind='bar', colormap='Accent', figsize=(8,6))\n",
    "        plt.gcf().autofmt_xdate()\n",
    "        plt.title('Mean Last 6 Months Return', fontsize=16)\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        ax.spines[\"top\"].set_visible(False)  \n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    \n",
    "    return complete \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare_gain(sf_ts, nyc_ts, name='San Francisco', name2='New York City')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this comparison data, New York is historically the best option to invest, regardless of the time-lag for the investment as it has provided the highest return rate and smaller loss when that is the case for all time options: 2, 5 and 10-year investments. It also requires less capital invested. \n",
    "\n",
    "However, the return in the last period of our data, the month of 2007 were higher for San Francisco. In any case, we want to model the returns data so that we can see what the prediction is for the future in terms of not only house price but rather of returns over investment, and if the trend is favorable to investing in San Francisco, despite the history showed by the data.\n",
    "\n",
    "\n",
    "\n",
    "### 5. Modeling\n",
    "\n",
    "Our stakeholders are interested in investing with a 2 and 5 year return, so we will only work with these two series for our modeling and prediction. In any case, predictions for over 5 years would be too far ahead and not reliable (have a too wide confidence interval).\n",
    "\n",
    "As a first step in our modeling process, we will create a function to calculate and plot both the Autocorrelation (ACF) and the Partial Autocorrelation (PACF) for our data, which will be helpful with some initial idea for difference order we might use, as well as the other parameters of our model. Let's also check our returns data for stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checking stationarity \n",
    "returns = [sf_roi2, sf_roi5, nyc_roi2, nyc_roi5]\n",
    "names = ['San Francisco 2-Year', 'San Francisco 5-Year', 'NYC 2-Year', 'NYC 5-Year']\n",
    "for ts, name in zip(returns, names):\n",
    "    plot_roll(ts, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our data is not stationary and this means we will need to difference it by at least order 1 in order to better fit a model. However, we note that the spike with the creation of new zipcodes in NYC in 2004 is causing our data to look very inconsistent and this might be prejudicial to our modeling. We want to remove this data impacted so as to not confuse our models, and work with only the years after such impact - 2006 for the 2-Year data, and 2009 for the 5-Year data. Let's remove this and plot the rolling statistics again for NYC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out inconsistent data for New York City\n",
    "nyc_roi2 = nyc_roi2['2006':]\n",
    "nyc_roi5 = nyc_roi5['2009':]\n",
    "\n",
    "# save variable again to make sure we have the current time series for NYC\n",
    "returns = [sf_roi2, sf_roi5, nyc_roi2, nyc_roi5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_roll(nyc_roi2, 'New York 2-Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roll(nyc_roi5, 'New York 5-Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " That is much better. Let's look at all ACF and PACF plots. We will create a function to take a time series and plot both ACF and PACF at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to plot ACF and PACF\n",
    "def acf_pacf(ts,alags=40,plags=40):\n",
    "    fig,(ax1,ax2) = plt.subplots(2,1,figsize=(14,8))\n",
    "    plot_acf(ts,lags=alags, zero=True,ax=ax1)\n",
    "    plot_pacf(ts,lags=plags, ax=ax2, method = 'ywmle')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checking ACF and PACF plots  \n",
    "for df, name in zip(returns, names):\n",
    "    print(f'\\n \\n {name} \\n')\n",
    "    acf_pacf(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see on our ACF plots that for most data there is a significant positive correlation up to around 15 lags, which is expected for this type of data such as real estate values. This might give us a hint that this number could be a good order for our MA parameter. \n",
    "\n",
    "On our PACF plots we see that we might be dealing with an AR order 4 or 5 for the San Francisco 2-Year Return data, and likely 1 or 2 for the others. \n",
    "\n",
    "We also want to decompose the data so that we can note if there is any seasonality that we can't observe clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# decompose time series\n",
    "for df in returns:\n",
    "    result = seasonal_decompose(df, model='additive')\n",
    "    result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like indeed there is some seasonality for all the data, so we will keep this in mind. We will work with a SARIMAX model to account for all factors. As a first step we will work with the data for San Francisco 2-year Returns for a test, and run an initial model as our base, using order 1 for all parameters and s=12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fit_sarimax_model(ts, order=(1,1,1), seasonal_order=(0, 0, 0, 12), summary=True, plot=True):\n",
    "    \n",
    "    '''Takes a time series and runs a SARIMAX model with parameter order provided. \n",
    "    If no parameter provided default is (1,1,1), (0,0,0,0). \n",
    "    Has the default option to print model summary and plot diagnostics, which can be turned off'''\n",
    "  \n",
    "    # fit model\n",
    "    model = sm.tsa.statespace.SARIMAX(ts,\n",
    "                                    order=order,\n",
    "                                    seasonal_order=seasonal_order,\n",
    "                                    trend='ct',\n",
    "                                    enforce_stationarity=False,\n",
    "                                    enforce_invertibility=False)\n",
    "    output = model.fit(d=0)\n",
    "    \n",
    "    if summary==True:\n",
    "        print(output.summary())\n",
    "\n",
    "    if plot==True:\n",
    "        # plot model diagnostics\n",
    "        output.plot_diagnostics(figsize=(15, 18))\n",
    "        plt.show()\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sf2_output = fit_sarimax_model(sf_roi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from our diagnostics that our residuals are not quite normally distributed, and show some correlation so that means our model is far from ideal and we must work to improve it. That makes sense since this was a basic, initial model.\n",
    "\n",
    "Let's get predictions and the RMSE for this model so that we have some comparison ahead. We will again create a function to make this step easily replicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_predictions(ts, model_output, steps=24, plot=True, show=True):\n",
    "    \n",
    "    '''Gets one-step-ahead forecast for model, \n",
    "    calculates Root of Mean Squared Error, \n",
    "    makes future predictions for number\n",
    "    of steps passed as parameter(default is 24), \n",
    "    plots results, \n",
    "    provides last forecasted value with confidence interval'''\n",
    "\n",
    "    \n",
    "    # get preditions from model for data period\n",
    "    pred = model_output.get_prediction(start='2016-01-01', dynamic=True, full_results=True)\n",
    "    conf = pred.conf_int()\n",
    "    \n",
    "    if plot:\n",
    "        #Plot observed and predicted values with confidence interval\n",
    "        ax = ts['2014':].plot(label='Observed')\n",
    "        pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.9)\n",
    "        ax.fill_between(conf.index,\n",
    "                        conf.iloc[:, 0],\n",
    "                        conf.iloc[:, 1], color='g', alpha=.2,\n",
    "                        label='Confidence Interval')\n",
    "        ax.set_ylabel('Return %')\n",
    "        plt.title('Observations vs Predictions')\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # compare real and predicted values to validade model and compute the rmse\n",
    "    predicted = pred.predicted_mean\n",
    "    real = ts['2016-01-01':].value\n",
    "    mse = mean_squared_error(real, predicted)\n",
    "    rmse = math.sqrt(mse)\n",
    "    \n",
    "    if show:\n",
    "        print(f'The RMSE of our forecast is {round(rmse, 2)}.' + '\\n')\n",
    "        \n",
    "    \n",
    "    # Get forecast and confidence interval for steps ahead in future\n",
    "    future = model_output.get_forecast(steps=steps, dynamic=True)\n",
    "    future_conf = future.conf_int(steps=steps)\n",
    "\n",
    "    if plot:\n",
    "        # plot results\n",
    "        ax = ts['2014':].plot(label='Observed', figsize=(12, 8))\n",
    "        future.predicted_mean.plot(ax=ax, label='Forecast')\n",
    "        ax.fill_between(future_conf.index,\n",
    "                        future_conf.iloc[:, 0],\n",
    "                        future_conf.iloc[:, 1], color='k', alpha=.25)\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Returns')\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # show prediction for end of step-period (in this case in 2 years future time)\n",
    "    forecast = future.predicted_mean[-1]\n",
    "    maximum = future_conf.iloc[-1,1]\n",
    "    minimum = future_conf.iloc[-1,0]\n",
    "    predictions = {}\n",
    "    predictions['forecast'] = forecast\n",
    "    predictions['maximum'] = maximum\n",
    "    predictions['minimum'] = minimum\n",
    "    \n",
    "    predictions = pd.DataFrame.from_dict(predictions, orient='index', columns=['Return at End of Forecast'])\n",
    "    \n",
    "    if show:\n",
    "        print(predictions)\n",
    "        \n",
    "    return forecast, maximum, minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sf2_predictions = get_predictions(sf_roi2, sf2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial model predicts that within 2 years from the end of our data, the return for investment for those who had invested 2 years prior would be of about 29%, but the wide confidence intervals put this return ranging from -7.6% to 66%.\n",
    "\n",
    "This is a large interval for our prediction and wouldn't help much our stakeholders with their decision. Let's try to fit a model using the parameters that we could infer from our ACF and PACF plots and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sf2_output2 = fit_sarimax_model(sf_roi2, order=(3, 1, 14), seasonal_order=(0, 0, 0, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions(sf_roi2, sf2_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our AIC is much smaller, but the RMSE is higher. This means we might possibly be overfitting. In addition, our model is very complex with so many lags. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the better order parameters for our model, we will use the function auto_arima, which performs a search over possible orders and helps us select the parameters that minimize a given metric. In this case we are using the AIC (Akaike Information Criterion) value to measure the quality of our model.\n",
    "\n",
    "We will input the maximum parameters that we could infer from our ACF and PACF plots, and leave the function to estimate automatically the differencing term.\n",
    "\n",
    "(You can learn more about the auto_arima [here](http://www.alkaline-ml.com/pmdarima/0.9.0/modules/generated/pyramid.arima.auto_arima.html).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to run auto arima and search for best model parameters\n",
    "def find_orders(ts, name='', show=True):\n",
    "    \n",
    "    '''Takes a time series and finds the best differencing order, as well as the other best\n",
    "    parameters for a SARIMAX model using auto_arima'''\n",
    "    \n",
    "    stepwise_model = auto_arima(sf_roi2, start_p=1, start_q=1,\n",
    "                               max_p=5, max_q=15, seasonal=True,\n",
    "                               m=12, test='adf', trace=False,\n",
    "                               error_action='ignore',  \n",
    "                               suppress_warnings=True, \n",
    "                               stepwise=True)\n",
    "    if show:\n",
    "        print(f'{name} - Order: {stepwise_model.order}, Seasonal Order\" {stepwise_model.seasonal_order}, AIC: {stepwise_model.aic()}')\n",
    "        \n",
    "    return stepwise_model.order, stepwise_model.seasonal_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_orders(sf_roi2, name='', show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's fit our model again using these parameters provided and see how we perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sf2_output3 = fit_sarimax_model(sf_roi2, order=(4, 0, 4), seasonal_order=(2, 0, 0, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model does seem better. We have a much lower AIC than our base, and the residuals look a bit more normally distributed. Let's verify what predictions and errors we get with this new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions(sf_roi2, sf2_output3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model indeed seems like it does a better job than our previous tries. The RMSE for our forecast is smaller and our prediction range is not as wide, with a 12% return predicted but possibly ranging from -10% to 34% at a 95% confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to do now is to streamline this process and apply it to all our time series so that we can compare which one is the best. Let's create functions to do all this at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to compare 2-year returns\n",
    "def compare_2y_roi(ts_list, names_list, plot=True):\n",
    "    \n",
    "    '''\n",
    "    Takes a list of time series and its names and returns forecasted returns, along with\n",
    "    confidence interval values for a 2-year in time prediction in a dataframe format.\n",
    "    '''\n",
    "    # create dictionary to store results\n",
    "    hist_roi = {}   \n",
    "    \n",
    "    # model and get predictions\n",
    "    for ts, name in zip(ts_list, names_list):\n",
    "        hist_roi[name] = {}\n",
    "        order, seasonal_order = find_orders(ts, show=False)\n",
    "        model = fit_sarimax_model(ts, order=order, seasonal_order=seasonal_order, summary=False, plot=False)\n",
    "        forecast, maximum, minimum = get_predictions(ts, model, steps=24, plot=False, show=False)\n",
    "        hist_roi[name]['Predicted Return'] = forecast\n",
    "        hist_roi[name]['Maximum Return'] = maximum\n",
    "        hist_roi[name]['Minimum Return'] = minimum\n",
    "    \n",
    "    # format data into dataframe\n",
    "    results = pd.DataFrame.from_dict(hist_roi)\n",
    "    \n",
    "    # plot results\n",
    "    if plot:\n",
    "        ax = results.plot(kind='bar', colormap='Accent', figsize=(8,6))\n",
    "        plt.axhline(y=0, color='red')\n",
    "        plt.title('Projected Returns after 2-Year Period', fontsize=16)\n",
    "        ax.spines[\"top\"].set_visible(False)  \n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        plt.gcf().autofmt_xdate()\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        \n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define another function for a 5-Year time return\n",
    "def compare_5y_roi(ts_list, names_list, plot=True):\n",
    "        \n",
    "    '''\n",
    "    Takes a list of time series and its names and returns forecasted returns, along with\n",
    "    confidence interval values for a 5-year in time prediction in a dataframe format.\n",
    "    '''\n",
    "    # create dictionary to store results\n",
    "    hist_roi = {}\n",
    "    \n",
    "    # model and get predictions\n",
    "    for ts, name in zip(ts_list, names_list):\n",
    "        hist_roi[name] = {}\n",
    "        order, seasonal_order = find_orders(ts, show=False)\n",
    "        model = fit_sarimax_model(ts, order=order, seasonal_order=seasonal_order, summary=False, plot=False)\n",
    "        forecast, maximum, minimum = get_predictions(ts, model, steps=60, plot=False, show=False)\n",
    "        hist_roi[name]['Predicted Return'] = forecast\n",
    "        hist_roi[name]['Maximum Return'] = maximum\n",
    "        hist_roi[name]['Minimum Return'] = minimum\n",
    "        \n",
    "    # format data into dataframe\n",
    "    results = pd.DataFrame.from_dict(hist_roi)\n",
    "    \n",
    "    # plot results\n",
    "    if plot:\n",
    "        ax = results.plot(kind='bar', colormap='Accent', figsize=(8,6))\n",
    "        plt.axhline(y=0, color='red')\n",
    "        plt.title('Projected Returns after 5-Year Period', fontsize=16)\n",
    "        ax.spines[\"top\"].set_visible(False)  \n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        plt.gcf().autofmt_xdate()\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_2year = [sf_roi2, nyc_roi2]\n",
    "names_2year = ['San Francisco', 'New York City']\n",
    "forecast_2years = compare_2y_roi(ts_2year, names_2year)\n",
    "forecast_2years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_5year = [sf_roi5, nyc_roi5]\n",
    "names_5year = ['San Francisco', 'New York City']\n",
    "compare_5y_roi(ts_5year, names_5year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very interesting result! The past data had shown us that New York City has been consistently a better city to invest in real estate when compared to San Francisco, regardless of the horizon of the return: 2, 5 or 10 years. However, the most recent data from our time series shows San Francisco returns above those of from NYC. We wanted to see if this was a trend that would be sustained in the future for our stakeholders, making San Francisco a more interesting investment option in despite of New York historical superior gains.\n",
    "\n",
    "What our model results show us is that if our stakeholders are looking to buy and sell within 2 years, San Francisco indeed sounds like the best option for investors that are not risk averse, with higher predicted and maximum outcomes. However, San Francisco's predicted minimum return is more than that of New York City. We can conclude that San Francisco offers more gain potential however presents higher risk. \n",
    "\n",
    "On the other hand, when the timespan for the investment extends to 5 years, the predictions for San Francisco are much more volative. There could be a loss as high as 70% and the predicted return is much smaller than that of New York City. The later shows a more consistent investment option, with high predicted gains and low risk when the time of investment is of 5 years.\n",
    "\n",
    "In conclusion:\n",
    "\n",
    "### **For a 2-Year Investment:** \n",
    "\n",
    "Agressive/Focus on higher potential gain: **San Francisco** <br>\n",
    "Conservative/Focus on smaller potential loss: **New York City**\n",
    "\n",
    "### **For a 5-Year Investment:** \n",
    "\n",
    "**New York City**\n",
    "\n",
    "Ok, now that we have some recommendations for our stakeholders in terms of which city to invest, let's move on to the second part of the project and find the best 5 zipcodes in each location to invest on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Best Zipcodes to Invest\n",
    "\n",
    "Besides the overal analysis comparing these two cities, we also want to recommend what are the best zipcodes to buy real estate in each place. For that we will create a dictionary with the time series data for all zipcodes from both cities so that we have access to the data for all zipcodes in order to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to include all zip code time series\n",
    "sf_zip_dict = {}\n",
    "    \n",
    "# Iterate over all the zip codes within each city\n",
    "for z in san_francisco.Zipcode.unique():\n",
    "    \n",
    "    # Choosing that specific zip code\n",
    "    temp_zip_df = san_francisco[san_francisco.Zipcode == z]\n",
    "\n",
    "    # Creating a time series (via melting) for that zip code\n",
    "    temp_zip_ts = melt_data(temp_zip_df)\n",
    "\n",
    "    # Adding that time series to a dictionary\n",
    "    sf_zip_dict[z] = temp_zip_ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we can now access each zip code by the keys\n",
    "sf_zip_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_zip_dict[94109].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also plot them\n",
    "fig = go.Figure()\n",
    "for i, key in enumerate(sf_zip_dict):\n",
    "    fig.add_trace(go.Scatter(x=sf_zip_dict[key].index, y=sf_zip_dict[key].value, name=str(list(sf_zip_dict.keys())[i-1])))\n",
    "fig.update_layout(title_text='San Francisco Zillow Home Value Index (ZHVI)',\n",
    "                      xaxis_rangeslider_visible=False, showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some zip codes did suffer more harshly from the crisis, which can indicate higher risk for investments. We can also note that the rising trend is more accute for some zip codes when compared to others, which could lead to higher ROIs for the short/medium timeframe. \n",
    "\n",
    "Let's do the same thing for the New York City zip codes, generating the time series for each one and saving it all into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat with nyc data:\n",
    "nyc_zip_dict = {}\n",
    "    \n",
    "# Iterate over all the zip codes within each city\n",
    "for z in nyc.Zipcode.unique():\n",
    "    \n",
    "    # Choosing that specific zip code\n",
    "    temp_zip_df = nyc[nyc.Zipcode == z]\n",
    "\n",
    "    # Creating a time series (via melting) for that zip code\n",
    "    temp_zip_ts = melt_data(temp_zip_df)\n",
    "\n",
    "    # Adding that time series to a dictionary\n",
    "    nyc_zip_dict[z] = temp_zip_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot zipcodes data all together\n",
    "fig = go.Figure()\n",
    "for i, key in enumerate(nyc_zip_dict):\n",
    "    fig.add_trace(go.Scatter(x=nyc_zip_dict[key].index, y=nyc_zip_dict[key].value, name=str(list(nyc_zip_dict.keys())[i-1])))\n",
    "fig.update_layout(title_text='New York Zillow Home Value Index (ZHVI)',\n",
    "                      xaxis_rangeslider_visible=False, showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting plot that we should never show to a stakeholder. In any case it reminds us that some NYC zipcodes were created later on. Let's remove all NaNs from our data so that we don't encounter any problems with our modeling further ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaNs\n",
    "for key in nyc_zip_dict:\n",
    "    nyc_zip_dict[key].dropna(inplace=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data\n",
    "nyc_zip_dict[11223].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, now we need a function to do the heavy lifting for us. We want to calculate the 2-Year and the 5-Year return for each of these zipcodes, and then select the 5 best. The criteria that we will use will be:\n",
    "\n",
    " - **highest predicted return**\n",
    " - **lowest risk - the least loss/higher minimum returns among the zipcodes with the highest returns**\n",
    " \n",
    " \n",
    "Let's start with that function, and we will use some of the functions that we have created previously as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_return_2year(dictionary_of_zipcodes, city):\n",
    "    \n",
    "    '''Takes a dictionary with time series for each zipcode key, and a city name. Calculates returns on investment\n",
    "    for a 2-year period lag. Model returns and make predictions for future returns. Selects zipcodes with highest\n",
    "    returns and smaller losses. Plots returns and returns dataframe displaying results.'''\n",
    "    \n",
    "    # calculate 2 year returns\n",
    "    returns = {}\n",
    "    for i, key in enumerate(dictionary_of_zipcodes):\n",
    "        temp_return = (dictionary_of_zipcodes[key] - dictionary_of_zipcodes[key].shift(periods=24))/dictionary_of_zipcodes[key].shift(periods=24)*100\n",
    "        temp_return.dropna(inplace=True)\n",
    "        returns[list(dictionary_of_zipcodes.keys())[i-1]] = temp_return\n",
    "        \n",
    "    # model and get predictions\n",
    "    results = {}   \n",
    "    for i, key in enumerate(returns):\n",
    "        results[list(returns.keys())[i-1]] = {}\n",
    "        order, seasonal_order = find_orders(returns[key], show=False)\n",
    "        model = fit_sarimax_model(returns[key], order=order, seasonal_order=seasonal_order, summary=False, plot=False)\n",
    "        forecast, maximum, minimum = get_predictions(returns[key], model, steps=24, plot=False, show=False)\n",
    "        results[list(returns.keys())[i-1]][' Current Return'] = round(returns[key].iloc[:-1].values.mean(),0)\n",
    "        results[list(returns.keys())[i-1]][' Predicted Return'] = forecast\n",
    "        results[list(returns.keys())[i-1]]['Maximum Return'] = maximum\n",
    "        results[list(returns.keys())[i-1]]['Minimum Return'] = minimum\n",
    "    \n",
    "    # format data into dataframe and filter desired results\n",
    "    results = pd.DataFrame.from_dict(results)\n",
    "    \n",
    "    results = results.transpose()\n",
    "    \n",
    "    results = results.nlargest(30, [' Predicted Return'])\n",
    "    \n",
    "    results = results.nlargest(5, ['Minimum Return']) \n",
    "    \n",
    "    results = results.nlargest(5, [' Predicted Return'])\n",
    "    \n",
    "    # plot results\n",
    "    ax = results.plot(kind='bar', colormap='Accent', figsize=(8,6))\n",
    "    plt.axhline(y=0, color='red')\n",
    "    plt.title(f'Best {city} Zipcodes to Invest for 2-Year Period', fontsize=16)\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_zips_2yroi = rank_return_2year(sf_zip_dict, city='San Francisco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_zips_2yroi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nyc_zips_2yroi = rank_return_2year(nyc_zip_dict, city='New York City')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_zips_2yroi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rank_return_5year(dictionary_of_zipcodes, city):\n",
    "    \n",
    "    '''Takes a dictionary with time series for each zipcode key, and a city name. Calculates returns on investment\n",
    "    for a 5-year period lag. Model returns and make predictions for future returns. Selects zipcodes with highest\n",
    "    returns and smaller losses. Plots returns and returns dataframe displaying results.'''\n",
    "    \n",
    "    returns = {}\n",
    "    \n",
    "    # calculate 5 year returns\n",
    "    for i, key in enumerate(dictionary_of_zipcodes):\n",
    "        temp_ts = (dictionary_of_zipcodes[key] - dictionary_of_zipcodes[key].shift(periods=60))/dictionary_of_zipcodes[key].shift(periods=60)*100\n",
    "        temp_ts.dropna(inplace=True)\n",
    "        returns[list(dictionary_of_zipcodes.keys())[i-1]] = temp_ts\n",
    "    \n",
    "    #eliminate data that is too small for forecast\n",
    "    for key in returns:\n",
    "        if(len(returns[key]))<=60:\n",
    "            del returns[key]\n",
    "\n",
    "    for key in returns:\n",
    "        if(len(returns[key]))<=60:\n",
    "            del returns[key]\n",
    "\n",
    "        \n",
    "    # model and get predictions\n",
    "    results = {}   \n",
    "    for i, key in enumerate(returns):\n",
    "        results[list(returns.keys())[i-1]] = {}\n",
    "        order, seasonal_order = find_orders(returns[key], show=False)\n",
    "        model = fit_sarimax_model(returns[key], order=order, seasonal_order=seasonal_order, summary=False, plot=False)\n",
    "        forecast, maximum, minimum = get_predictions(returns[key], model, steps=60, plot=False, show=False)\n",
    "        results[list(returns.keys())[i-1]][' Current Return'] = round(returns[key].iloc[:-1].values.mean(),0)\n",
    "        results[list(returns.keys())[i-1]][' Predicted Return'] = forecast\n",
    "        results[list(returns.keys())[i-1]]['Maximum Return'] = maximum\n",
    "        results[list(returns.keys())[i-1]]['Minimum Return'] = minimum\n",
    "       \n",
    "    # format data into dataframe and filter desired results\n",
    "    results = pd.DataFrame.from_dict(results)\n",
    "    \n",
    "    results = results.transpose()\n",
    "    \n",
    "    results = results.nlargest(30, [' Predicted Return'])\n",
    "    \n",
    "    results = results.nlargest(5, ['Minimum Return']) \n",
    "    \n",
    "    results = results.nlargest(5, [' Predicted Return'])\n",
    "    \n",
    "    # plot results\n",
    "    ax = results.plot(kind='bar', colormap='Accent', figsize=(8,6))\n",
    "    plt.axhline(y=0, color='red')\n",
    "    plt.title(f'Best {city} Zipcodes to Invest for 5-Year Period', fontsize=16)\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_zips_5yroi = rank_return_5year(sf_zip_dict, city='San Francisco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_zips_5yroi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nyc_zips_5yroi = rank_return_5year(nyc_zip_dict, city='New York City')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_zips_5yroi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our stakeholders were interested in investing in real estate either in San Francisco, CA or in New York City, NY. They were looking to find our what was the location that would offer better ROI over a 2-year and a 5-year time invested.\n",
    "\n",
    "We conducted a time series analysis and modeling based on data from Zillow Research Page, which provides us with the mean Zillow Home Value Index for several years and many zipcodes from both cities. \n",
    "\n",
    "Based on our analysis of historical data and on predictions obtained from our models, we would recommend the following:\n",
    "\n",
    "### **2-Year Investments:**<br>\n",
    "\n",
    "#### Agressive Investor Profile - Focus on maximum gains: **San Francisco**<br>\n",
    "   \n",
    "   \n",
    "#### Conservative Investor Profile - Focus on minimizing losses: **New York**<br>\n",
    "   \n",
    "   5 Best San Francisco Zipcodes for 2-year Investment:  <br>\n",
    "       - \n",
    "       - \n",
    "       - \n",
    "       - \n",
    "       - \n",
    "   \n",
    "   5 Best New York City Zipcodes for 2-year Investment:  <br>\n",
    "       - \n",
    "       - \n",
    "       - \n",
    "       - \n",
    "       - \n",
    "   <br>\n",
    "   \n",
    "### **5-Year Investments:** city name <br>\n",
    "\n",
    "   \n",
    "   5 Best San Francisco Zipcodes for 2-year Investment:  <br>\n",
    "       - \n",
    "       - \n",
    "       - \n",
    "       - \n",
    "       - \n",
    "   \n",
    "   5 Best New York City Zipcodes for 2-year Investment:  <br>\n",
    "       - \n",
    "       - \n",
    "       - \n",
    "       - \n",
    "       - \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future work we could explore other cities, as well as continue to improve our model by trying to deepen our understanding of the data and trends and continue to tweak and improve our prediction model. We could also experiment with other types of time series models such as Vector Autoregression Moving-Average with Exogenous Regressors (VARMAX), or Holt Winter’s Exponential Smoothing (HWES) for example.\n",
    "\n",
    "We could also gather more recent data so that our predictions better reflect the current state of real estate in the areas of interest, since the data we worked with goes only up to April 2018.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
